DP Architecture:
2 components: Scan Engine and Scans Stats each with its own API (2 endpoints -> RESTful APIs)

Scan Engine  => the PUBLISHER
    -> gives a verdict to a file based on a dummy logic  
    -> file hash (md5) % 3 ({0 -> clean, 1 -> malware, 2 -> PUA}) 
    -> scan results documents (JSON) will be stored in a mongo db -> this is the db #1
        -> format_db_1:
        {
            hash: string (file hash)   ==> mongo identifier in this case
            dev_id: string (device id, uniquely identifying a device)
            // ft : string (filetype  - exe, elf, jpg etc) - get this info from file magic bytes? -> yes nice idea
            size : int (file size)
            verdict : int (detection verdict) -> {0, 1, 2}
            created : datetime (date when this file was first seen => inserted in db) -> first seen time
        }
    -> in order not to stress the db we will use a redis cache (LRU?)
        -> so the requests will initially look in redis for answers and only if not present there
        then they will be initially written in db and then in redis
        -> redis format: file_hash(str): verdict(int)
        -------
        -> when a POST request is sent (a file should be scanned), the following steps will be triggered:
            1. redis is queried for the file md5
                md5 key present     ==> it means that we have an mongo doc for this file => dummy_logic does not trigger, we just update the ls (last seen time) field
                                        of the corresponding document in mongo
                md5 key not present ==> it means that this is the first time we see this file => dummy logic triggers and a verdict is obtained => then a mongo document is created 
                                        => also a redis key-value pair is inserted
        -> when a GET request is sent (we want the verdict for a specific file hash), the following steps will be triggered:
            -> first we query redis for the file hash key -> if present we return the document from mongo (we know it's there)
                                                          -> if not, we tell the user to upload the file first and come back later :)
        --------
    -> finally the scan results (mongo docs) will be continuously posted to the queue and from here the subscriber (Scans Stats) will consume them

    1. File Scan API    ->   the API for the Scan Engine component
        -> the user uploads a file and receives a verdict regarding the malicious state of the file
        -> will have 1 route (/scan) and accept 2 types of requests here (GET & POST):
            -> POST /scan: submit a file for scanning
            -> GET /scan: retrieve a scan result

Scans Stats => the SUBSCRIBER
    -> no interaction with the client
    -> job that runs in the background and stashes telemetry
    -> computes and stores in db: 
        -> total infected and clean scans
        -> unique file (hashes) scans
    -> those statistics are continuously written to the overview document
    
    -----------------------
    -> here we have another mongo, db #2 => holds device scans overview data
        -> format_db_2:
        {
            dev_id: string (device id, uniquely identifying a device)
            u_s: int (unique scans, any verdict)
            u_i: int (unique infected, pua included)
            u_c: int (unique clean)
        }
    -> the Scan Stats Worker consumes data from the queue (docs in format_db_1) and populates db 2 (with its redis included)
    -> redis #2 format: -> maybe sth like this: 3 keys corresponding to the same device
        {dev_id}_1 -> u_s
        {dev_id}_1 -> u_i
        {dev_id}_1 -> u_c
        fiecare key sa fie un bloom filter de fapt

    2. Scans API
        -> only exposes /overview (GET)
            -> here the user can request the scans status of a given device (by giving it the device id)
            -> then a mongo document will coresponding to that device will be returned
            
    